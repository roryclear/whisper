 If this started, the company update. My red box is very stock. We're in a lot of red box. We set out the little cloud. There's five red box. Five new red box is for internal use. We have a new one. I'm 15. I'm sure I have contact with all the business. I'll see what we can get. 5. Yeah. What's the last one? Yes. What's the last one? What? I don't know. What? I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I'm pretty good at reading this week. Finally got. Multi and gradient merge that depend on you ops and not on anything else. We have a separate class called multi lazy buffer. Multi lazy buffer doesn't exist anymore. It's constructed at schedule time. Now by pushing this ops multi down through the graph. So it's pretty much the same logic, but instead of being a class, it's a graph for your right. And then once multi was done, I could finally merge gradient. The problem with gradient with multi lazy buffer is you'd have to apply the gradient on the multi lazy buffer. So you have to apply the gradient basically before you apply the multi, but yeah, this is the problem now, because the multi stuff is applied at scheduled time in the gradient, that's before scheduled time. So we got a big, we got like a 200 line reduction with that. And all this stuff in functions gone, functioned up high, the functioned class has gone. And I think there's still quite a bit of cleanup left to do. Just random stuff left around for all gradient. That can just go. So yeah, I'm very happy with them. And it seems like the U-op project that we started in, like October is finally coming to a close in the work. So I'm just going to do that. Okay. Cool. And dimensions, some scheduling or things that you're working on. Oh, so I merged this morning. I merged the. Just a moving can take us everything is finally good enough that we get five in your MOOC and take you to a semester D, which is cool. And I added in like one more case that was like, okay, for the assigned. Yeah, I think I think I'm going to start working on is the. So, for all, like we have this thing break scared right now and like we then break scared into these pieces and then we reconstruct a graph and do a BFS on my graph. I think I can make something that looks a lot like linearized but for the kernels and the end up with the graph and you'll also end up with the buffers pointing in. So this would happen before bufferization. Everything going to kernel and then the inputs to that kernel are either other kernels or buffers. And I think I can do that. Yeah, in the same way, linearized works. So something to start experimenting with. I think I'll work on that for the rest of the week or I have that or I can start working on the DSP stuff. Okay. Oh, speaking of. In the blog reports channel, I think one of the previous linearizing like compile. Errol now it's a. I can finish loop in block maybe you're interested in that. Ah, that's very well. 53. 53 is now an infinite loop in block. I think previously was an issue for some variables gop and. Probably become a different issue in block. Oh, yeah, look at them. Great. So with the new multi a new gradient and. It has consequences to rest net and first. So I think for rest net. The main issue is our old hyper parameters doesn't converge anymore. I'm not for why. So we have two separate issue. One is the loss scalar on flow 16 and needs to change otherwise. I mean, flow 16 training was brittle with loss. Scaler anyway, I think we just have some numerical issue. Now, like with I can update the loss. So it converges similar to flow 32. So I think let's find but for flow 32 training. It didn't converge. I'm not sure if it's because. It's very sensitive to some hyper parameters or layers. Box. It might be bugs before and now it's fixed or something like fix and now has a bug. I'm not sure which way. Let's rest net. Yeah. Sorry. I'm curious. I'm curious what the change is and we need to find a way to to test and quantify this kind of stuff. Yeah. So it's very good. The only change I could find was the value thing. But the value thing should change the number of kernels but I don't think it should ever paint the correct as like it shouldn't change it numerically. Yeah. So my point is maybe there was a bug. Oh, you mean you cannot think of any other difference in terms of compute before and after. Well, the sigmoid one but we dealt with that. It wouldn't you know what it might be it might be the sigmoid like I went through all the derivatives and was like okay I've heard what what's different about the new derivatives and the older it would have been really you have some obvious difference. But other. We're trying to sigmoid to a new sigmoid a while ago and resin that was fine after that change. We didn't actually we changed sigmoid to the oh wait no we did no we did you're right we removed it. Yes, I don't know what else that's. Yeah, I think so I was confused. I think what I would do is probably. So I would wrong like separate wrong in the background but I would probably start to look into the sea forcing and see if there's like if I think sea forest fast for to iterate. Yeah, I mean see for us just seems to work. Yeah, but it we've proved it last year we when we were tweaking and improving resin that we also have similar moments that some change seems to be fine on sea for but. Because the hyper parameter and training epochs are pretty aggressive on resonance we are very likely to on just on some edge of weird stuff. So if any end this is just some hyper parameter tuning I won't be worrying too much. What we will see on it. I don't just have a change. Yeah, let's do. Do you want to try to buy second and see if you can find it like I looked through. Let's do also schedule our task. I. Yeah, I can buy second more and pretty sure it's changed. Oh, change the multinaot gradient. Let's I need to separate because I run like several rounds and. I can buy second this I'm pretty sure it changed right after multi I don't know if it changed more after gradient. But initially I thought it might be like because we changed behavior to less sink and sink edge norm and more if we really change behavior. Here's a test change. Which I didn't think it would be heavier. No, that test thing with the strides thing it's just because of number of strides. It should be the same behavior. You change something of all the real. I didn't mean to. At least you disable one test that I don't fully understand I can look at later. Oh, I disabled the test that used to with sir that doesn't assert anymore. Yeah, so that's I understand really ever why that asserted yeah what I did make a change. I changed it so all things being not real or okay. Okay, then that's probably fine. Then it's just a empty maybe or is the rose. Yeah, it should be all zeros. Yeah. So if that's. I don't know. I sure I would buy second more and post the findings on the I think for rest net if I can find if I can understand this better maybe to some see for thing I think is fine or fine. Well, when I say is not conversion is also not off by that much. So it's not like completely off. Yeah, I really got yeah, I really got the bug but we just we just had to have a better way of talking about numerical stability. Did we ever merge in the the thing where we put the accumulator first? Yes, that was merger while we go. That one that should be helpful. That definitely changes things too, but I would expect all the less before before these and the rest now was fine. Oh yeah. Yeah, pretty much every every single I change kernel after that around the rest net. So pretty familiar was like what change stuff and what did it. Okay. So for birds. Birds has the issue with dropout dropout has an issue with run like run like has an issue because we cannot Fx axis has the construct like the tenser level. You have suggestions. Yeah. Similar to I think I think for the zero like or for like this is. It was as similar to maybe less important but it's still annoying. I can't sit different. No, because now we have a lot more memory than either. If it's not, yeah, sure. No, I don't have an obvious way to fix that. Okay. I was thinking about this. Yeah, because this we need this for a completely burnt wrong. So bird is fine. Birds. We kind of know dropout was not really needed. Runs still runs now if we disable dropout but we need that back. Yeah, and I guess I see why you aren't ran like and dropout. Yes, I was the proposed solution initially for dropout. Yeah, you could like multiply this need by. I don't want one. Yeah, one one ways probably instead of run like resolving everything. Make it like reuse the you up three fry and resolve it in ops or something. I don't I don't like that. Why you want to you want to. Resolve and get it ops might be okay. Oh, you mean resolve access to ops. Yeah, I've that's probably going to go at probably the way to go. Move some of that multi logic and yeah into ops. I think this is what I suggested. Yeah, move the multi logic and do ops and then. Yeah, something like that. I need I really want this to be similar to how we handle was the was the constant like the zero likes and once like. Yeah, yeah, no, I don't think this should be a one-off thing. I think that moving logic makes a lot of sense. Just having like doubt access actually work. Yeah, yeah, so that's probably what I will be watching on this week fixing the M.O.Purf models and the dropout and these counts like I think. You can move on to. I was doing a rear. I have moved on to drivers first. Yeah, so for the driver. I think that takes this problem. So actually, I am driver can now just restore from all states and we don't. Leave the GPU. Innovate state anymore. So yeah, also I fuzzy so yeah, it works now. And yeah, we also have now. We also can now collect some metrics from the GPU. So yeah, I'm just going to fix these idle power consumption. So some reason, GFX activities at 100 right now all the time. So memories. Good note. Maybe something with power gate and so you'll double check that. Yeah, I think you have to manually turn the core off. I think it's just a simple thing, but. Yeah. And it's, yeah, okay, I'll take a look into that. Yeah, I like MSI. Yeah, also. Can we just for several days, re-plug. 7600. First remember, the PCI for me to test it. And. Oh, we can work to use be. Yeah, I think it will be just fast for me to test it. That everything works. Yeah, on. Yeah, so I'm in Asia right now. I can't plug it in somewhere else. But we can definitely get that set up. But the, I left the other computer still the same. The first computer is still a 7900 X DX. Yeah, and. I think the first thing to do is to get it working there. And then also get it working on the other chip. Okay, yeah. But yeah, I will definitely get a computer with a 7900 with a 7600 PCI. Before it's time for that. Eagle. Oh, I have a question. So is AM good to use for say, ResNet. For some reasons, ResNet's now hands on tiny 13, the red box. And I can either. You try to use AMC if we got better error message from that or if you have any other suggestions. Um, Project. And what kind do you mean? They want you share it in empty hardware. Oh, no, they're on more. It just crashed. On the runtime errors on the on device hand. So I just I just got that I run I had to run last night and run run run run run this morning. Bose hand after I don't 20 minutes. I can share more in the channel. Just want to know if running with AM would give better error message. Okay, no, No, I'm not sure if we have any better message, but it's interesting to see why it's hands. Am D. No, not to him. Yeah, so for now is. Yeah, I found. Yeah, go go. Okay, I mean, I'll put more in the. Yeah, I'm the channel. If I can like reproduce it. Yeah, okay. Yeah, I was just. Yeah, so I think I found one. But I'm not really sure about and to post it. So that's page file to the address. Like have. Zero zero zero zero zero. So actually, I think it's just from his D may like you got it on the. AM and I just tried to and actually it's kind of reproducible or not only on AM, but also on AMD. It actually the same address. I know I think maybe something were related. I'm not sure. So yeah, it's actually it's reproducible running a lot of. I think how it's called to reduce benchmark with something like that. But yeah, without jeet without graphs. So yeah, and also. Yeah, also this week I'm just going to see. I had to disable. I did this bind method for SD may. It currently worked only for a M and I see for a snadow. It also calls. Hanks for some reason. So without it, it works fine. I've tested it with Burton with Burton. It worked. So I don't know. It's just pretty reproducible. There is a snat. So. Something is different. I don't get device hand for red box for training bird. It's really just. But I'm not sure why. But it's not your. You are on this. I'll probably share one more about the rest net hand because let's different from the one I post before. But since you are already aware of the issue, I'll be glad to you. I'm probably just using the green books. My guess would be that it has something to do with local memory. There's there's not stuff uses local memory a lot more than though the birth stuff. Like a whole lot of the kernels use local memory. Mm-hmm. I don't know. It could be lots of everything too. It could be like the communication pattern or who knows. But yeah, I mean, if it's happening in both drivers and it's probably a bug in our. In our runtime. Oh, Okay. I just don't have a 7600 tape. So we'll get that in the computer at some point. Great. Sounds good. Okay. So I'll driver. Let's go back to scheduler. Hello. Hello. I see you amused. But if you are talking, I can not hear you. Hey. I can hear some background noise. Hello. Hello. Is it good now? Yeah. Yeah. It's good now. Okay. I looked into the kernel stopper bit. I just think we need to move a bunch of these stuff to the new block style. I think your just going to work on that side. I want to have the complex and stuff. Are you planning to do the kernel refactor? I think you know this part best. Like how will our works and everything? Yeah. So I think there's a few prerequisites that I'm not exactly sure. Well, so I think definitely let's get. Do you not construct unmaster valid merged? Sure. Yeah. I'll get that merge this week. And the little being more lower work. And then I'll get the original schedule or schedule which just matches the views. Yeah. The kernel thing. So right now we're doing bufferization. Like I think before we even think about changing breaks get. We have to stop creating buffers for things that we never realize. Like right now we create all of these buffers and then don't use a lot of them. So do you want to split the work as to I do the refactor for removing all those buffersations. Yeah. Well, I think I think I should probably work on the DSP stuff as well. I don't exactly see how to like I can write the kernel thing. But the problem is I don't know where to like stop consuming with the kernels. But we already have a lot of logic for that. I don't want to rewrite the logic at the same time that I'm rewriting like the style. So I think maybe if you could first refactor bufferization to happen after choosing. But actually doing the DSP stuff. What reduce after? The group realization stuff that they just like. After group realizes yeah yeah and it's okay for group realizes to be custom. I mean I think that okay like here's my proposal for how to do it. So before bufferization do group realizes then after group realizes insert. Contiguences everywhere that you want there to actually be a buffer. Then I can write kernel which comes in and grabs everything up to a contiguous. Then do bufferization. Like what the thing that I imagine this eventually being is not something that really ever has a bufferization step. It becomes the graph of the kernels with the kernels either pointing like every source of a kernel is either another kernel or a. I'm not a buffer I'm treated. But yeah so do you think it's doable with what we currently have to move. Buffization after. Group realizes. I mean sure but. Like this big I merged your ad merge views to the. Of folding. And it's like. I upstream it and it works just it's green and I merge it and it's like because tensor mapping this right so I think like there's this. That we could take of just removing bufferization and going right away to the. Like style of. Being the fusion stuff with graph right and grabbing all the you up and gathering that. That would just like make this problem not exist. Is that an option or do we need to go this half. The only. Yeah. We could try it okay I'll you know what I will try it tomorrow. I will I will. Take a shot at just writing the kernel thing and see and how far gets. Without the current logic and we'll see like what we end up getting will see if I get something that's correct. Yeah kind of like kind of like tuning that. And then yeah we'll see if that's the one the right that seems reasonable. Also how's the moving of multi to tensor map going. Yeah I close it's it's there's this problem that bufferization has which is that you can track views you can only track basis. If you have a multi and then a reshape. And then you push that reshape over to the to the over to the sources of the multi you lose that multi so it's like. You have to be fixed with some sort of like more fundamental refactor. I mean I can try to like redo this whole thing again if it's like really annoying to have that multi maps. Well but like yeah I think having a multi maps that exposes. I don't know champion what is it about it that it can't be on the graph or I'm not. I don't know if you just say that if there's a reshape. Is it shaped? Yeah views are just not tracked. The views stay views movements of always stay movements of people we only track basis right now in these get your letter. Because only basis how far first right so only basis are tracked. I added that thing this week of like the constant so tensor can be conconced if you do a multi roll. And you schedule it it become the constant. Yeah yeah. But like you can't do the same with the reshape right you can't make the reshape. You view after schedule it stays reshape. Yeah I mean it has to stay reshape. And you lose that reshape so I sort of like I had the comments of like the last thing before I close it. Like can we go from a view back to the movements of I don't think. No. But it's like you have to you kind of have to do this right you have to if you want to realize a view afterwards where you want to map a tensor to a view from a base. But you can think about what the cast before we use doing cast before view is making a base tensor a view tensor. And you can't stack a view on top of it you have to actually chain all the movements of the exactly the way that you did it's in the. Oops. Well yeah I mean you sure have to so the reason that you can't have views in the tensor stuff is you can't compute the gradient of a view. I mean I got the air. Yeah yeah there's there's no way to do it I spent I spent like half a day on this. But I don't know just like why there should ever be views. Okay thought experiment cast before view. If it's not actually. Is this. No it's not I mean cast before view is a is a misdomer it's cast before movement of and yeah you just move the cast before the movement of some replay the same movement of some top of the cast. How do you know what the movements of star after graph rate map is done. Like that's a problem right you have to some you have to have some sort of way to look at the view. Oh we can speak the movement of from that view you kind of know it but you have to track it. I see. Yeah. I mean why does graph rewrite map have to collapse the views. I guess there's a lot of stuff we want to do now. Okay I see the problem. I don't have an obvious solution right now. I'll start working on all I'll work on the kernel stuff tomorrow and we'll see how far I dealt with it. I'll probably similar to any grad like get it to a point where it's like kind of correct and then head back to you. I'm also working on this. He can expect a new new ways coming. Oh it's speed. With speed. It's already working. It's just a very ugly but it renders beautiful and this. I'll make it print. Oh then the other thing that I want to talk about is was a memory profile. I think I think this can wait for another time. I like the visit is going to be fast. Yeah this is going to be fast GPU accelerated. Great. Let's move on to Bounties. I'm working with TensorFlow. Hi. Hello. For search over shape. I'm implementing a tip name today. There has been already a discussion on the full request. But nothing new to add from my side. I also started starting the new TensorFlow course. What I see they introduced many things that may require incremental support. Like there is a new TensorFlow memory unit that has that it supports. My also being fast AMX. Another thing is that they share some requirements with the H100 TensorFlow course. So progress on any of those will benefit both. Also from my part I started starting about. I'm starting about the new precision framework. I imagine it's similar to how we count flopps or memory. But quite more complicated. I tried to come up with a proof of concept in the incoming weeks. But also I need a lot of research and setting from my part. I know that the functionality series created in like you want to keep your stuff in. Basically more registered than GPUs have or AMX. You know, the main reason GPUs have TensorFlow is not FMA units. Is Vickers tensor course have symmetric inputs and outputs. And this works nicely for GPU registers and warps. Whereas like the AMX doesn't, right? The AMX has this accumulator that's that's the square. That's like the accumulator is on square and the registers are in by the center thing to see GPUs starting to move toward something else. Yep. Is 5767 good merge? Yeah, sorry. I don't know the number. I'm going to look. Oh, you tip nine. Oh, yes. That's only the rename. Yes, it's good to merge. Yep. Cool. Great. Okay. Uh, we have any update on AMX. I think I merge a several PR. Oh, okay. I don't know. No worries. Oh, it looks nice. Oh, that lowers on the here internet stuff. I think I merge the rule thing. Just let me know if any review or PR is blocking. I will review your stuff. Cool. Right. Yeah. Yes. Um, so for directing the net, I got the training loop done. I'm working on the evil now. I was initially thinking earlier to kind of optimize for the speed on the evaluation, but I just wanted to get the correctness done. So I think what I'll do is, I think the model evil had some sort of a runner on the resident side of things that kind of used the data loader. So I'm going to just kind of try it on on separate branch to make that faster. And then see what kind of speeds I get, but then, but then get the full training and validation working first to get the full correctness. And then, yeah, so that's kind of my goal this week to get that working. I was just rebased to master because as you know, there are some issues or difference on the rest net. So you might want to rebate some new things. Another thing about data loader is I noticed that I'm not sure why, but for the rest net, the most of the speed regression seems to come from data load data fetch time is now very slow. I think now our per step is 240 milliseconds and data fetch itself is like 160. And I used to be 50. So just making sure when you migrate to the data loader, see if it's really faster. I think like that. Okay, sounds good. And which machine will be will you be using? I think we got a lot more red so I can move to other machine. If you are using red, it should have enough machine and not block by waiting on a machine or stuff. But just post it the any box access channel if you don't have a machine. Yeah, I was on tiny, I was on a green box, I think, but just recently, just because I think tiny, 10 and tiny, 13, we're getting used, I think when I check the activity. So I can, yeah, I can go to another machine of possible. We will figure something out. I can move out of 13. And so you can have 13. Yeah, we have 130, 130, 130, 130, too. Still already be accessible to everyone at the at the company. I think we can give them to external people too. If we could just have one. I think I can move all of 13 and 13 can be used for retin on that training. Okay, that's good. Uh, tiny chat browser. I'm not sure if you have permission to talk. Where's that? Is it hooped? What's there discord now? Oh, yeah, I was going to talk about it. You are a speaker now. Hello. You're near me? Yes. Cool. Um, so, you know, it's been working on web GPU for a couple of weeks. I just got working on awesome with playing today, but it's not hosted yet. Um, so, you know, my priority has been to try to get it to work on a phone because I think we'll get the most exposure and people will get the most value if they just, you know, see the tweet. They click on the link and it just works without having to go and figure out what web GPU is, you know, just think about like the average person, right? So, um, so that's the plan. It's, you know, it's only taking up two gigabytes of memory with, with, uh, with playing because I'm using quantized weights. Um, and I just want to get that hosted, see how fast it is, just see if it actually works on a phone. I want to see if I can get, um, the web GPU memory, just, if I can shrink that, uh, using quantized, uh, weights if possible. Um, and, uh, you know, I'd like to have, I'd like to have both web GPU and playing work from the same link, you know, where just kind of detects what you have enabled and just uses the fastest and possible. So, uh, you know, that's what's going on now. Let me, let me have a, let me have a, let me have a few questions. Oh, uh, and you just go over the bottom. Like what's considered done? Oh, the bounty specifies specifically both. Uh, tiny chat and browser supporting, playing and web GPU. Okay. And it's a thousand dollars. Do you, you have a definition for support, like how fast should it be? Work reasonably well. Again. Yeah. So, yeah. So, I made your, I made your blue, so you can talk in the, in the channel to cool. Thanks. Yeah. Like, it's, it's a roughly 15 to 20 tokens per second, on web GPU, um, you know, on more of my, yeah. I'm on a 3080. So, that's probably why. But so, so that seems pretty, pretty. Oh, okay. Yeah. But, but that seems pretty, pretty decent. Maybe we can get it better. But playing is like less than a token per second. Just kind of sketch. But, you know, if that's what it takes to make it work on a phone, I'm still interested in it. But maybe we can get it faster. I just need to play with it. Yeah. I mean, I'd expect it to be more than a token per second, right? You just like gave the rambo up the system and divide it by the size of what you're accessing. I would imagine playing good find that no problem. Unless something else is wrong. Yeah. I, I barely played with it. I just got it working. Like, I want to just just think of it. Cool. Yep. Sounds like a good progress. Oh, X is window CI. I don't know if the person's here or is it a different person. I saw we marge an impo window CI. Yeah. We merge windows CI. It turned out to be really simple. Because the L of the M back end just kind of worked. So yeah, I put another bounty up for plan and you put it up for it. But I don't have to, I don't have to. I don't have to murder the maintain a cough loader. Hopefully, another one to do it. Okay. Do we have any further plans for windows or after let us stop this kind of mint tent? I think it's maintained. I think that like if we can just do test time, there's many bad things that we can get. It seems pretty good. Okay. Like CL that comes on me. Cool. It's a way to get that on the work. Yeah. That sounds pretty good. Okay. Oh, and add it. If that's an add-nall because I saw the person working on it was in a meeting, but I don't see him anymore. I'll probably read the new change and comment on the PR. Oh, we missed Graffery right to point out. Oh. Okay. Sorry Graffery right to point out. Oh, I don't know if I'm going to find him here. Oh, he doesn't he left and rejoin. If you're trying to speak, I can not hear you. I got a river. I got a river. The renown. I broke something. Oh my fabulous woman. Shake and thank you keep not trying to go. Just, yeah. Okay. we have any bunkies or things that is all right what happened to LLVMB flow system cast to red was in render a lot it's locked i think it's closed i think i just don't like where the i kind of don't like where the function this thing okay yeah i think that's pretty much it i'm gonna ask where can we watch the recording of these meetings it's in the recordings channel yeah let's i think that's everything on the agenda cool i'm super tired tonight i'm still adjusting till time let me know if this time is too late for you and we can figure something out i should be fine once i had you missed but maybe sounds good cool i think that's it for this meeting thanks everyone and see you next week