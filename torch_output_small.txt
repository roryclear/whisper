 Okay, started. The company update. Only red box is there in stock. We have a lot of red boxes. We set up a little cloud. There's five red boxes for internal use. We have a lot of red boxes. We have updates on 59s early. We have contact to all of them. We'll see what we can get. Nothing else, right? I want to talk about new multi and new gradient. I had a pretty good week this week. We finally got multi and gradient merge that depend on Uops and not on anything else. We had a separate class called multi-lazy buffer. It doesn't exist anymore. It's constructed at schedule time by pushing this ops multi down through the graph. It's pretty much the same logic, but instead of being a class, it's a graph rewrite. Once multi was done, I could finally merge gradient. The problem with gradient with multi-lazy buffer is you'd have to apply the gradient on the multi-lazy buffer. You have to apply the gradient before you apply the multi. This is no problem now because the multi-stuff is applied at schedule time and the gradient happens before schedule time. We got a 200 line reduction for that. All the stuff in functions gone, function.py, the function class is gone. I think there's still quite a bit of cleanup left to do. Just random stuff left around from all gradient that can just go. I'm very happy with them. It seems like the Uop project that we started in October is finally coming to a close in Orc. Cool. You mentioned some scheduler things that you were working on. Oh, so I merged this morning. I merged the just removing contigues. Everything is finally good enough that we get finally remote contiguous from SGD, which is cool. I added one more case that was okay for the assign. I think what I'm going to start working on is the kernel. We have this thing break-sked right now. We then break-sked into these pieces and then we reconstruct a graph and do a BFS on that graph. I think I can make something that looks a lot like linearized but for the kernels and then you'll end up with the graph and you'll also end up with the buffers pointing in. This would happen before bufferization. It would just capture everything in a kernel and then the inputs to that kernel are either other kernels or buffers. I think I can do that in the same way linearized works. Something to start experimenting with. I think I'll work on that for the rest of the week. I have that or I can start working on the DSP stuff. Speaking of block, in the bug reports channel, I think one of the previously linearizing compile arrow, now it's an infinite loop in block. Maybe you were interested in that. Yes, earlier 53. 53 is now an infinite loop in block. I think previously it was an issue for some variable scope and probably become a different issue in block. Well, yeah, look into it. Great. So with the new multi and new gradient and it has consequences to ResNet and Perth. So I think for ResNet, the main issue is our old hyperparameters doesn't converge anymore. I'm not sure why. So we have two separate issue. One is the loss scalar on flow 16. I need to change otherwise. I mean, flow 16 kernel was brittle with loss scalar anyway. I think we just have some numerical issue now with I can update the loss scalar. So it converges similar to flow 32. So I think that's fine. But for flow 32 training, it didn't converge. I'm not sure if it's because it's very sensitive to some hyperparameters or there's bugs. It might be bugs before and now it's fixed or it's something that's fixed and now has a bug. I'm not sure which way. Let's ResNet. Yeah. I'm curious. I'm curious what the change is and we need to find a way to test and quantify this kind of stuff. Yeah. That's where I go ahead. The only change I could find was the relu thing. But the relu thing should change the number of kernels, but I don't think it should ever change the correctness. Like it shouldn't even change it numerically. Yeah. The problem is maybe there was a bug. You mean you cannot think of any other difference in terms of compute before and after? Well, the sigmoid one, but we dealt with that. You know what it might be? I went through all the derivatives and was like, okay, what's different about the new derivatives and the old derivative? And relu has an obvious difference. But other than that. We changed sigmoid to the new sigmoid a while ago and ResNet was fine after that change. We didn't actually. We changed sigmoid to the, oh wait, no we did. No we did. You're right. We removed it. Yes, I don't know what else it is. So I was confused. I think what I would do is probably, so I would round like separate round in the background, but I would probably start to look into the C4 thing and see if there's like, if, I think, some of the things that I could do. C4 is faster to iterate. Yeah, I mean C4 still seems to work. Yeah, but it, we prove it last year we, when we were tweaking and improving ResNet, we also have similar moments that some change seems to be fine on C4, but because the hyper parameter and training epochs are pretty aggressive on ResNet, we are very likely to just on some edge of weird stuff. So if at the end this is just some hyper parameter tuning, I won't be worrying too much. We will see on that. I don't understand what change. Yeah, that too. Do you want to try to bisect it and see if you could find it? Like I looked through all the scheduler tasks. Yeah, I can bisect it more. I'm pretty sure it's changed on the. Oh, it changed on multi, not gradient. Let's, I need to separate because I run like several rounds and I can bisect this. I'm pretty sure it changed right after multi. I don't know if it changed more after gradient. Initially, I thought it might be like, because we change behavior to let's think, unsync, catch, no, I don't know if we really change behavior. There's a test change. Which I didn't take a behavior. No, that test change with the strides thing. It's just because there's no more strides. It should be the same behavior. You change something about a real. I didn't mean to. Oh, at least you disabled one test that I don't fully understand. I can look at it later. Oh, I disabled the test that used to assert that doesn't assert anymore. Yeah, so that's I understand really ever why that asserted. Yeah, what I did make a change. I changed it so all things being not real or okay. Okay, then let's probably find that it's just empty maybe or zeros. Yeah, it should be all zeros. Yeah. So if that's I will bisect more and post the findings on the channel. I think for ResNet, if I can find, if I can understand this better, maybe I do some C for thing. I think it's fine. When I say it's not converging, it's also not off by that much. So it's not like completely off. Yeah, I really doubt. Yeah, I really doubt there's a bug, but we just we just had to have a better way of talking about numerical stability. Did we ever merge in the thing where we put the accumulator first? Yes, that was merge a while ago. That one, that should be helpful. That definitely changes things too, but I would expect I would expect all the other before these and ResNet was from. Yeah. Yeah. Pretty much every every single change kernel after that around ResNet. I'm pretty familiar with like what changed stuff and what did. Okay. So for birds, birds has the issue with dropout. Dropout has an issue with Rennlike. Rennlike has an issue because we cannot have X axis at the construct, the tensor level. Do you have suggestions? Yeah. Similar to I think I think four is zero like or four like this is those are similar issue. It might be less important, but it's still annoying. The constant is different. Because now we have a lot more memory than needed. If it's none, yeah, sure. No, I don't have an obvious way to fix that. Okay. I think about this. Yeah, because this we need this for a completely bird wrong. So birds is fine birds. We kind of know dropout was not really needed. Wrong still runs now if we disable dropout, but we need that back. Yeah. And I guess I see why you want Rennlike and dropout. Yes, that was the proposed solution initially for dropout. Yeah, you could like multiply this to get by. So I think one way is probably instead of Rennlike resolving everything, make it like reuse the. You up three-frame resolve it in ops or something. Yeah. I don't I don't like that. What you want to you want to resolving it in ops might be okay. Oh, you mean resolve access in ops. Yeah, that's probably the way to do it. Probably the way to do it is like move some of that multi logic and yeah into ops. I think that's what I suggested. Yeah, move the multi logic into ops and then. Yeah, something like that. I need to I really want this to be similar to how we handle with the with the counts like the zero likes and ones like. Yeah, yeah, no, I don't think this should be a one-off thing. I think that moving logic makes a lot of sense. Just having like dot access actually work. Yeah. Yeah, so that's probably what I will be working on this week fixing the ML perv models and the. Dropout and these cons like thing. You can move on to. I was looking earlier. I moved on to drivers first. Yeah, so for the driver. Think I fixed this problem. So actually I am driver can now just restore from all states and we don't leave the GPU in a big state anymore. So yeah, also a fuzzy so yeah, it works now. And yeah, we also have now we also can now collect some metrics from the GPU. So yeah, I'm just going to fix this idle power consumption. So for some reason, GFX activities at 100 right now all the time to memory is good. Maybe something with power gating. So yeah, double check that. Yeah, I think you have to I think you have to manually turn the cores off. I think it's just a simple thing, but. Yeah. Yeah. And it's yeah, okay, I'll take a look into that. Yeah, I like a MSI. Yeah, also. Can we just for several days, reply. 7600. First, remember into PCI for me to test it. And. Back to USB. I think it will be just faster for me to test it. That everything works. Yeah, on yeah. So I'm in a year right now. I can't plug it in somewhere else, but we can definitely get that set up. But the I left the other computer still the same. The first computer is still a 7900 XTX. Yeah, yeah, yeah. I think the first thing to do is to get it working there and then also get it working on the other chip. Okay, yeah. But yeah, I will definitely get a computer with a 7900 with a 7600 PCI before it's time for that. Okay, cool. I have a question. So is AM good to use for say ResNet? For some reasons, ResNet now. Hands on tiny 13, the red box. And I can either. Use try to use AMC if we got better error message from that or if you have any other suggestions. So what do you mean? The one you shared in the hardware? No, there are more. It just crashed. And the only mess error message I get is the on the runtime errors on the device hand. So I just got that I run I run the device hand. So I just I just got that I run I had one run last night and went around this morning both hands after 20 minutes. I can share more in the channel. Just want to know if running with AM would give better error message. Okay, no. No, I'm not sure that we have any better message. But okay, it's interesting to see why thanks. I mean, you know, AMD or not AM. Yeah, so for now is yeah, I found. Yeah, good. Okay, I'll post more in the AMD channel. See if I can like reproduce it. Yeah, okay. Yeah, so I think I found one bug. I'm not really sure about and you posted it. So that's page faulted the address like have zero zero zero. So and actually, I think it's just from as DMA. Like you got it on the AM. And I just tried to and actually it's kind of reproducible. Not only on AM but also on AMD. Actually the same address. I know I think maybe it's something where related. I'm not sure. I think it's just from the AM. I think it's just from the AM. Where related. I'm not sure. So yeah, it's actually it's reproducible running a lot of our. I think how it's called reduce benchmark or something like that. Yeah, without it without graph. So yeah. And also, yeah, also this week, I'm just going to see I had to disable. These bind method for as DMA. It currently worked only for AM and I see for a snaddle it also calls Hanks for some reason. So without it, it works fine. I've tested it with Bert and with Bert it's it worked. So I don't know if it's just pretty reproducible. The rest net. Something is different. I don't get device hand for red box for training birds. It's really just rest net, but I'm not sure why. But since you are you are on this. I'll probably share one more about the rest net hand because that's different from the one I post before, but since you are already aware of the issue. You've led to you probably just use ring bucks. My guess would be that it has something to do with local memory because the rest net stuff uses local memory a lot more than the birds. Like a whole lot of the kernels use local memory. I don't know. It could be lots of other things too. It could be like communication pattern or who knows. But yeah, I mean if it's happening in both drivers and it's probably a bug in our, you know, runtime. Our driver. Okay, so. 7600. We'll get that the computer at some point. Okay, so driver. Let's go back to scheduler. Hello. Hello. I see you a mute. But if you are talking, I cannot hear you. Hi. Hi. Hi. I can hear some background noise. Hello. Hello. Is it good now? Yeah. It's good now. Okay. So I looked into the kernel stuff a bit. I do think we need to move a bunch of these stuff to the new block style. I think George is going to work on that. So I don't want to have the complex and stuff. Are you planning to do the kernel refactor? I think you know this part best. Like how block works and everything. Yeah. So I think there's a few prerequisites that I'm not exactly sure. Well, so I think definitely let's get do not construct unmasked valid merged. Sure. Yeah, I'll get that merged this week. That ended up being more lower work than actual scheduler schedule. It just merges the views. So. Yeah. The kernel thing. So right now we're doing bufferization. Like I think before we even think about changing break sked, we have to stop creating buffers for things that we never realize. Like right now we create all of these buffers and then don't use a lot of them. So do you want to split the work as to I do the refactor for removing all those bufferizations and then. Yeah, well, I think I think I should probably work on the DSP stuff as well. The. I don't exactly see how to like I can write the kernel thing, but the problem is I don't know where to like stop consuming with the kernels. But we already have a lot of logic for that. I don't want to I don't want to rewrite the logic at the same time that I'm rewriting like the style. So I think maybe if you could first refactor bufferization to happen after. choosing. But after doing the reduce of stuff. What reduce of stuff. The group realizes stuff that exists like. After group realizes yeah yeah yeah and it's okay for group realizes to be custom I mean I think that okay like here's here's my proposal for for how to do it. So before buffering nation do group realizes then after group realizes insert contiguous is everywhere that you want there to actually be a buffer. Then I can write kernel which comes in and grabs everything up to a contiguous. Then do bufferization. Like. What the thing that I imagine this eventually being is not something that really ever has a bufferization step. It becomes the graph of the kernels with the kernels either pointing like every source of a kernel is either another kernel or a. A buffer a pre created buffer. Pre created. Yes do you think it's doable with what we currently have to move. Bufferization after. Group realizes. Me sure but. Like this big I merged your ad merge views to the ops folding. It's like. I upstream it and it works just it's green and I measure this like because tensor map exists right so I think like there's this step that we could take of. Just removing bufferization and going right away to the. Like style of. In the fusion stuff with graphy right and grabbing all the ups and gathering. That would just like make this problem not exist. Is that an option or do we need to go this half. Dealing with. Yeah. We could try it. Okay. I'll you know what I will try it tomorrow I will I will. Take a shot at just writing the kernel thing and seeing how far it gets. Without the current logic and we'll see like what we end up getting we'll see if I get something that's correct. Yeah kind of like kind of like to me grad. And then yeah we'll see if that's the one of the way that seems reasonable. Also how's the moving of multi to tensor map going. Yeah I close that it's it's there is this problem that buffers Asian has which is that you can track views you can only track basis. If you have a multi and then a reshape. And then you push that reshape over to the chill it over to the sources of the multi you lose that multi so it's like. It has to be fixed with some sort of like more fundamental refactor. I mean I can try to do that. Like redo this whole thing again if it's like really annoying to have that multi map slip. Well, but like yeah I mean. I think having the multi map step exposes. Like I don't understand yet. What is it about it that it can't be on the graph. I meant. As you say that if there's a reshape. Yeah the views are just not tracked the views stay views movements of always stay movement of we all we only track basis right now in the schedule after authorization because only basis have buffers right so only basis are tracked. I added that thing this week of like the const stuff so tensors can become const if you do a mall zero and you schedule it become the cons. But like you can't do the same with the reshape right you can't make the reshape. If you after schedule it stays reshape. Yeah I mean it has to stay reshape. And you lose that reshape so I sort of like I had the comments of like the last thing before I close it can we go from a view back to the movement tops. I don't think no. But it's like you have to you kind of have to do this right you have to if you want to realize a view afterwards where you want to map a tensor to a view from a base. Like you can think about what cast before we use doing cast before view is making a base tensor a view tensor. And you can't stack a view on top of it you have to actually chain all the movements of the exact the way that you did it's in the ups. Well yeah I mean you should have to so the reason that you can't have views in the tensor stuff is you can't compute the gradient of a view. Yeah yeah there's there's no way to do it I spent I spent like half a day on this. But I don't understand like why there should ever be views. Okay thought experiment cast before view. What's not actually. Is this. No it's not I mean cast before view is a is a misnomer it's cast before movement tops. And yeah you just move the cast before the movement tops and replay the same movement tops on top of the cast. How do you know what the movements of star after graph rate map is done. Like that's a problem right you have to some you have to have some sort of way to look at a view. Reconstruct the movement tops from that view you kind of know it but you have to track it. I see. Yeah. I mean why does graph rewrite map have to collapse the views. I guess there's a lot of stuff we want to do. Okay I see the problem. I don't have an obvious solution right now. I'll start working on that. I'll work on the kernel stuff tomorrow and we'll see how far I dealt with it. I'll probably similar to any grad like get it to a point where it's like kind of correct and then hand it back to you. Okay I'm also working on this. You can expect a new new this coming soon. I'm going to start with a new view. I'm also working on this. You can expect a new new this coming soon. Oh with speed. With speed. It's already working. It's just very ugly but it renders beautiful and this. I'll make it pretty. Oh then the other thing that I want to talk about about this was a memory profiler. I think this can wait for another time. I like that this is going to be fast though. Yeah it's going to be fast. GPU accelerates it. Great. Let's move on to bounties. Starting with TensorFlow course. Hi. Hello. For search over shape I'm implementing a tip name today. There has been already a discussion on the pull request but nothing new to add from my side. I also started starting the new TensorFlow course. For what I see they introduced many things that may require incremental support. Like there is a new TensorFlow memory unit that has that it supports my also bring fast AMX. And another thing is that they share some requirements with the H100 TensorFlow course. So progress on any of those will benefit both. Also from my part I started starting about starting thinking sorry about the new precision framework. I imagined it similar to how we count flops or memory but quite more complicated. I'll try to come up with a proof of concept in the incoming weeks. But also I need a lot of more research and studying from my part. The new which framework? Precision in numerical stability framework. Oh cool. Yeah, no. Yeah, the 1590 stuff is interesting. The new TensorFlow course and I agree it's very similar to AMX. It's kind of where things have to end up. Like you want to keep your stuff in basically more registered than GPUs. The main reason GPUs have TensorFlow course and not FMA units is because TensorFlow course has symmetric inputs and outputs. And this works nicely for GPU registers and warps. Whereas like the AMX doesn't. The AMX has this accumulator that's the square. That's like the accumulators and squared and the registers are N. But it's interesting to see GPUs starting to move toward. Something else. Yep. Is 57 is 8767 good to merge? Yeah, sorry. I don't know the number. I'm going to look. Oh, you tip nine. Yes, that's only the rename. It's good to merge. Yep. Cool. Great. Yeah. Yeah. Yeah. Yeah. Yeah. Cool. Okay. Okay. Do you have any updates on on X? I think I merged several PR. Okay. No, no worries. Okay. Looks nice. So no worries on the your internet stuff. I think I merged several things. Just let me know if any review or PR is blocking me. I will review your stuff. Cool. Writing on that. Hey, guys. I'm going to go ahead and do a quick review. I'm going to go ahead and do a quick review. I'm going to go ahead and do a quick review. Okay. Okay. So for the internet, I got the training loop done. I'm working on the evil now. I was initially thinking earlier to kind of optimize for the speed on the evaluation, but I just wanted to get the correctness done. So I think what I'll do is I think the model evil had some sort of a runner on the resident side of things that kind of used the data loader. So I'm going to just kind of try it on a separate branch to make that kind of a better result. I'm going to try to get the full training and validation working first to get the full correctness. And then, yeah, so that's kind of my goal this week to get that working. I would suggest rebase to master because as you know, there are some issues or difference on rest net. So you might want to rebase on new things. Not necessarily on the rest net, but I would suggest that you might want to rebase on new things. Another thing about data loader is I noticed that I'm not sure why, but for rest net, the most of the speed regression seems to come from data load. Data fetch time is now very slow. I think now our per step is 240 millisecond and data fetch itself is like 160. And that used to be, I don't know, like 50. So just making sure when you migrate to the data loader, see if it's really faster and things like that. OK, sounds good. And which machine will you be using? I think we got a lot more red so I can move to other machine. If you are using red, you should have enough machine and not block by waiting on the machine or stuff. Just post in the tiny box access channel if you don't have a machine. Yeah, I was on a tiny, I was in a green box, I think, but just recently, just because I think tiny 10 and tiny 13 were getting used, I think, when I checked the activity. So I can go to another machine if possible. We will figure something out. I can move out of 13 and so you can have 13. Yeah, we have 30, 31 and 32 should already be accessible to everyone at the company. I think we can give them to external people too. But if we could just have some of these. Yeah, I think I can move out of 13 and 13 can be used for the same thing. Right now I'm not turning. That's good. Tiny chat browser. I don't know if you have permission to talk. Who is that? Is it hooked? What's their discord name? A hooped. Yeah, you want to talk about it? Yeah, I'm not sure if you are a speaker now. Hello. Can you hear me? Yes. Cool. So, you know, it's been working on WebGPU for a couple of weeks. I think it's been working on Lawson with Clang today, but it's not hosted yet. So, you know, my priority has been to try to get it to work on a phone because I think we'll get the most exposure and people will get the most value if they just, you know, see the tweet, they click on the link and it just works without having to go and figure out what WebGPU is, you know, just think about like what's going on. So, that's the plan. It's, you know, it's only taking up two gigabytes of memory with Clang because I'm using quantized weights. And I just want to get that hosted, see how fast it is, just see if it actually works on a phone. I want to see if I can get the WebGPU memory, if I can shrink it using quantized weights, if possible. And, you know, I'd like to have both WebGPU and Clang work from the same link, you know, where it just kind of detects what you have enabled and just uses the fastest thing possible. So, that's what's going on now. Let me know if you have any questions. What's the initial scope of the bounty, like what's considered done? The bounty specifies specifically both. Tiny Chat and browser supporting Clang and WebGPU. And it's $1,000 bounty. Do you have a definition for support? Like how fast should it be? Work reasonably well. Yeah, so. I made you a blue so you can talk in the channel too. Cool, thanks. Yeah, like it's roughly 15 to 20 tokens per second on WebGPU. You know, on my. Yeah, I'm on a 3080, so that's probably why. But so that seems pretty. Oh, okay. Yeah, but that seems pretty decent. Maybe we can get it better, but Clang is like less than a token per second, just kind of sketch. But, you know, if that's what it takes to make it work on a phone, I'm still interested in it. But maybe we can get it faster. I just need to play with it. Yeah, I mean, I'd expect it to be more than a token per second, right? You just like take the Ram Band with the system and divide it by the size of what you're accessing. I would imagine Clang could find that no problem. Unless something else is wrong. Yeah, I barely played with it. I just got it working like I want to just just tinker with it. Cool. Yep. Sounds like good progress. Thanks. Oh, Access Windows CI. I don't know if the person's here or it's a different person. But I saw we merge a simple Windows CI. Yeah, we merge Windows. Yeah, it turned out to be really simple because the LLVM backend just kind of worked. So, yeah, I put another bounty up for playing and he put a PR up for it, but I don't want to have to, I don't want to merge and maintain a call floater. Hopefully, there's another way to do it. Okay. Do we have any further plans for Windows or after that is done is kind of maintained? I think it's maintained. I think that like if we can just do test time with as many back ends as we can get, it seems pretty good. Okay. Excel back end will be cool. That's the way to get that one to work. Yeah, that sounds reasonable. Okay. And I added Flatten at null because I saw the person working on it was in a meeting, but I don't see him anymore. I'll probably read the new change and comment on the PR. Oh, we missed graph rate right 2.0. Oh, okay. Sorry, graph rate right 2.0. Oh, I don't know. He, I think he left and rejoin. If you're trying to speak, I cannot hear you. Oh, I got a river. I got a river. The rename. Okay. Shape. Negative number. First. Yeah. Okay. Okay. We have any bunty's or things. That's his owner. Oh, what happened to LLVM B flow 16 cast to rewrite rules and render the lock. Oh, it's yet it's locked. I think it's close. I think I just don't like where the account is. I don't like where the function. Okay. Yeah, I think that's pretty much it. Someone asked where can we watch the recording of these meetings. It's in the recordings channel. Yeah, I think that's pretty much it. Yeah, I think that's pretty much it. Yeah, I think that's pretty much it. Yeah, I think that's pretty much it. So yeah, that's pretty much it. Okay, so the recording of these meetings, it's in the recordings channel. Yeah, let's. I think that's everything on the agenda. I'm super tired tonight. I'm still adjusting to the time. Let me know if this time is too late for you and we can figure something out. It should be fine once I adjust. Sounds good. Cool. I think that's it for this meeting. Thanks everyone. See you next week.